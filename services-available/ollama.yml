networks:
  traefik:
    external: true

# description: Easy way to run large language models locally - Nvidia GPU
# https://hub.docker.com/r/ollama/ollama
# https://github.com/ollama/ollama

# add models to etc/ollama/Ollamamodels and run 
# make.d/scripts/ollama-update-models.sh
# to pull or update models
# see services-scaffold/ollama/Ollamamodels.sample for examples
# Available models: https://ollama.com/library/


services:
  ollama:
    image: ollama/ollama:${OLLAMA_DOCKER_TAG:-latest}
    env_file:
      - ./services-enabled/ollama.env
    container_name: ${OLLAMA_CONTAINER_NAME:-ollama}
    restart: ${OLLAMA_RESTART:-unless-stopped}
    networks:
      - traefik
    volumes:
      - ${OLLAMA_DATA_PATH:-./media/ollama}:/root/.ollama
      - /etc/localtime:/etc/localtime:ro
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ}
    ports:
      - ${OLLAMA_PORT:-11434}:11434
    labels:
      - joyride.host.name=${OLLAMA_CONTAINER_NAME:-ollama}.${HOST_DOMAIN}
      - traefik.enable=${OLLAMA_TRAEFIK_ENABLE:-true}
      - traefik.http.routers.ollama.entrypoints=websecure
      - traefik.http.routers.ollama.rule=Host(`${OLLAMA_CONTAINER_NAME:-ollama}.${HOST_DOMAIN}`)
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - com.centurylinklabs.watchtower.enable=${OLLAMA_WATCHTOWER_ENABLE:-true}
      - autoheal=${OLLAMA_AUTOHEAL:-true}
